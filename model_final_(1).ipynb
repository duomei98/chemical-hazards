{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Igz-UobUtxQ"
      },
      "source": [
        "#heading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ3iz9FOM1ME",
        "outputId": "cef68c47-bdc1-44d8-d529-e4edb0edaca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: could not create work tree dir 'chemical-hazards': Permission denied\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/duomei98/chemical-hazards.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8cUhCzVNBmY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from sklearn.metrics import accuracy_score\n",
        "import keras_tuner\n",
        "\n",
        "def precision(y, yhat):\n",
        "    if len([x for x in yhat if x == 1]) == 0:\n",
        "        return 0\n",
        "    ytot = np.column_stack((y, yhat))\n",
        "    return len([x for x in ytot if x[0] == 1 and x[0] == x[1]])/len([x for x in yhat if x == 1])\n",
        "\n",
        "def recall(y, yhat):\n",
        "    ytot = np.column_stack((y, yhat))\n",
        "    return len([x for x in ytot if x[0] == 1 and x[0] == x[1]])/len([x for x in y if x == 1])\n",
        "\n",
        "def f1(y, yhat):\n",
        "    return 2*precision(y,yhat)*recall(y,yhat)/(precision(y,yhat)+recall(y,yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU--YWVIN9n9"
      },
      "outputs": [],
      "source": [
        "!pip install deepchem\n",
        "from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
        "tokenizer = BasicSmilesTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi3dvwJ9NYUm"
      },
      "outputs": [],
      "source": [
        "overalldf = pd.read_csv('final-data-csv.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjpIa3FLN1O1"
      },
      "outputs": [],
      "source": [
        "overalldf['len'] = overalldf['SMILES'].apply(lambda x: len(tokenizer.tokenize(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V3EuWLcN0C2"
      },
      "source": [
        "We're going to be training on molecules with SMILES string length of 100 tokens or less, which roughly corresponds to the ~1 kilodalton size of a \"small molecule\" and should cover most commonly used reagents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbYysspLNza5"
      },
      "outputs": [],
      "source": [
        "shortdf = overalldf[overalldf['len'] <= 100]\n",
        "del overalldf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbMJl_kkNnEj"
      },
      "source": [
        "generating a mapping to reduce the tokens later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB3opg17jxPm",
        "outputId": "ac2819c9-0370-45e9-975f-bb69df77815f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'SMILES-enumeration'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 80 (delta 9), reused 24 (delta 7), pack-reused 49\u001b[K\n",
            "Receiving objects: 100% (80/80), 356.97 KiB | 3.08 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/EBjerrum/SMILES-enumeration.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TS9vSyxjxPm",
        "outputId": "3547b49b-d5bb-419c-f809-9449e2acc172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/yxie10/SMILES-enumeration\n"
          ]
        }
      ],
      "source": [
        "%cd SMILES-enumeration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPvZJ-OOjxPm"
      },
      "outputs": [],
      "source": [
        "from SmilesEnumerator import SmilesEnumerator\n",
        "sme = SmilesEnumerator()\n",
        "shortdf = shortdf.drop(columns=['len','CID'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu5-ftaqjxPm",
        "outputId": "5c0b59d1-a49e-4630-b39a-67888a1cded0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "explosive                  207\n",
              "flammable                13258\n",
              "oxidizer                   434\n",
              "pressurized                297\n",
              "corrosive                34656\n",
              "toxic                    15080\n",
              "irritant                185519\n",
              "health hazard            14979\n",
              "environmental hazard     17703\n",
              "dtype: int64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sum(shortdf.drop(columns=['SMILES']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrtwdV8yjxPm"
      },
      "source": [
        "We're going to add alternate smiles for the explosive, oxidizer, and pressurized hazard classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKlGux5ejxPm"
      },
      "outputs": [],
      "source": [
        "#repeats is the number of times we attempt to add another smile\n",
        "def addSmiles(hazards, repeats, shortdf):\n",
        "    smiles = set()\n",
        "    newdf = [shortdf]\n",
        "    for hazard in hazards:\n",
        "        print(hazard.upper(), ':\\n')\n",
        "        comps = shortdf[shortdf[hazard] == 1]\n",
        "        smiles.update(comps['SMILES'])\n",
        "        for comp in comps.iterrows():\n",
        "            #print(comp[1][0])     #uncomment to see which smiles is ducking up the code\n",
        "            try: #some SMILES are not compatible with the smiles randomizer, we'll ignore those\n",
        "                alt = sme.randomize_smiles(comp[1][0])\n",
        "            except:\n",
        "                print(comp[1][0], \"isn't quite valid\")\n",
        "                continue;\n",
        "            for i in range(repeats):\n",
        "                #print(np.transpose(np.array(comp[1].to_numpy())))\n",
        "                row = pd.DataFrame(np.transpose(comp[1].to_numpy()[:,np.newaxis]), columns=comp[1].index)\n",
        "                #return row\n",
        "                #print(comp[1][0])\n",
        "                alt = sme.randomize_smiles(comp[1][0])\n",
        "                if alt not in smiles:\n",
        "                    smiles.add(alt)\n",
        "                    #print(alt)\n",
        "                    row['SMILES']=alt\n",
        "                    newdf.append(row)\n",
        "    return pd.concat(newdf, axis=0)\n",
        "\n",
        "shortdf = addSmiles(['explosive', 'oxidizer', 'pressurized'], 100, shortdf)\n",
        "shortdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OljDO2PVjxPm"
      },
      "outputs": [],
      "source": [
        "#for saving the data to avoid processing over and over\n",
        "#shortdf.to_csv(path_or_buf='../augmented-final-data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzyY5voAPPg9"
      },
      "source": [
        "converting the SMILES into a one-hot matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gZZyYcEjxPn",
        "outputId": "4649ba9c-db44-48c4-a622-e4b9165e4406"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "explosive                10554\n",
              "flammable                18120\n",
              "oxidizer                 19162\n",
              "pressurized               2126\n",
              "corrosive                42727\n",
              "toxic                    19888\n",
              "irritant                208832\n",
              "health hazard            23378\n",
              "environmental hazard     25957\n",
              "dtype: object"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sum(shortdf.drop(columns=['SMILES']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uInTTFJKjxPn"
      },
      "outputs": [],
      "source": [
        "shortdf['len'] = shortdf['SMILES'].apply(lambda x: len(tokenizer.tokenize(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA7sqlIZjxPn",
        "outputId": "ed9c18cc-06a9-4836-f8f0-984d06fbb87e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>248584.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>31.790023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>16.325799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>27.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>39.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 len\n",
              "count  248584.000000\n",
              "mean       31.790023\n",
              "std        16.325799\n",
              "min         1.000000\n",
              "25%        21.000000\n",
              "50%        27.000000\n",
              "75%        39.000000\n",
              "max       100.000000"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shortdf.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n8k3vcoNsfo"
      },
      "outputs": [],
      "source": [
        "tokens = set()\n",
        "for smile in shortdf['SMILES']:\n",
        "  tokens.update(tokenizer.tokenize(smile))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXoU1p23Nf3m"
      },
      "outputs": [],
      "source": [
        "#dictionary of ions: raw tokens corresponding to final ion/ligand/whatever i'm not a chem person category\n",
        "# ion: charge (n/+/-) + type (metal/organic)\n",
        "iondict={}\n",
        "organics = ['B', 'Br', 'C', 'Cl', 'F', 'H', 'I', 'N', 'O','P','S']\n",
        "for token in tokens:\n",
        "  if token[0] == '[' and token[-1] == ']':\n",
        "    if token[1:-1] in organics or token[1:-1].upper() in organics:\n",
        "      continue\n",
        "    mo = 'o' if re.search('[a-zA-Z][a-z]?', token)[0] in organics or re.search('[a-zA-Z][a-z]?', token)[0].upper() in organics else 'm';\n",
        "    charge = '-' if token.find('-') >= 0 else '+' if token.find('+') >= 0 else 'n'\n",
        "    iondict[token] = charge+mo\n",
        "iondict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWOz-13PPJNH"
      },
      "outputs": [],
      "source": [
        "#obtaining the data we'll be directly using for our model\n",
        "# input: smiles converted into a vector of one-hot encoded (ish) vector\n",
        "type2idx = {  # key:\n",
        "    'nm': 0,  # neutral metal\n",
        "    '+m': 1,  # positively charged metal\n",
        "    '-m': 2,  # negatively charged metal\n",
        "    'no': 3,  # neutral organic\n",
        "    '+o': 4,  # positively charged organic\n",
        "    '-o': 5   # negatively charged organic\n",
        "}\n",
        "def untoken(token):\n",
        "  tok = np.zeros(22);\n",
        "  #if ion:\n",
        "  if token[0] == '[':\n",
        "    if token[1:-1] in organics:\n",
        "      tok[organics.index(token[1:-1])+11] = 1\n",
        "      return tok\n",
        "    iontype = iondict[token]\n",
        "    tok[type2idx[iontype]] = 1;\n",
        "  elif token == ')':\n",
        "    tok[6] = -1\n",
        "  elif token == '(':\n",
        "    tok[6] = 1\n",
        "  elif token.isdigit():\n",
        "    tok[7] = float(token)/10\n",
        "  elif token == '#':\n",
        "    tok[8] = 1\n",
        "  elif token == '=':\n",
        "    tok[9] = 1\n",
        "  elif token == '.' or token == '-':\n",
        "    tok[10] = 1\n",
        "  else:\n",
        "    if token in organics:\n",
        "        tok[organics.index(token)+11] = 1\n",
        "    else:\n",
        "        tok[organics.index(token.upper())+11] = 1\n",
        "  return tok\n",
        "\n",
        "def unsmile(smile):\n",
        "  vec = np.zeros(shape=(100,22))\n",
        "  tokens = tokenizer.tokenize(smile)\n",
        "  for i in range(len(tokens)):\n",
        "    vec[i] = untoken(tokens[i])\n",
        "  return vec\n",
        "\n",
        "smilesnp = np.zeros(shape=(len(shortdf['SMILES']), 100, 22))\n",
        "i=0\n",
        "for smile in shortdf['SMILES']:\n",
        "  smilesnp[i] = unsmile(smile)\n",
        "  i +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTAEIXgPPjr4",
        "outputId": "b78a86bb-89c9-40b2-e122-456e0f6bb14b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "explosive                10554\n",
              "flammable                18120\n",
              "oxidizer                 19162\n",
              "pressurized               2126\n",
              "corrosive                42727\n",
              "toxic                    19888\n",
              "irritant                208832\n",
              "health hazard            23378\n",
              "environmental hazard     25957\n",
              "dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = shortdf.drop(columns=['SMILES','len'])\n",
        "np.sum(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTo-qwe0jxPn",
        "outputId": "5c533bfb-4a78-4cfa-f741-30b9fc8ab3e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(248584, 100, 22)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "smilesnp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iwoDAFBjxPn"
      },
      "outputs": [],
      "source": [
        "np.save('../aug-smilesnp.npy', smilesnp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVaVJ9m4QK8e"
      },
      "outputs": [],
      "source": [
        "del tokens\n",
        "del shortdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAZ_fL_XjxPn"
      },
      "outputs": [],
      "source": [
        "#loading from before...\n",
        "#shortdf = pd.read_csv('augmented-final-data.csv')\n",
        "#smilesnp = np.load('aug-smilesnp.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTWLBX5bQJLx"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(smilesnp, y, test_size=0.2, random_state=21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsW6jVgtPkDW"
      },
      "source": [
        "Model building and training time:\n",
        "(and deleting extra variables to save ram because colab hates me :( )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ-yKr9TjxPn",
        "outputId": "96844a07-5e83-4b48-bea3-b4162923995f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(198867, 100, 22)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial model building/trying out ANNs vs CNNs"
      ],
      "metadata": {
        "id": "erCAEdeHkU6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec5OiFnOR40b",
        "outputId": "c0233f2b-03e2-4ebf-d50a-8b0ba94169e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yxie10/.local/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5844 - loss: 0.0773\n",
            "Epoch 2/10\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.6606 - loss: 0.0612\n",
            "Epoch 3/10\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.6792 - loss: 0.0561\n",
            "Epoch 4/10\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.6937 - loss: 0.0526\n",
            "Epoch 5/10\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.7025 - loss: 0.0504\n",
            "Epoch 6/10\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7086 - loss: 0.0486\n",
            "Epoch 7/10\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.7126 - loss: 0.0471\n",
            "Epoch 8/10\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7131 - loss: 0.0462\n",
            "Epoch 9/10\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7169 - loss: 0.0448\n",
            "Epoch 10/10\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7180 - loss: 0.0438\n"
          ]
        }
      ],
      "source": [
        "#building vnn (vanilla neural net) model\n",
        "#this turned out not to work so well...\n",
        "#VNNACTIVATIONFXN = 'tanh'\n",
        "#vnn = Sequential()\n",
        "#vnn.add(Input(shape=(2200,)))\n",
        "#vnn.add(Dense(units = 10, activation = VNNACTIVATIONFXN))\n",
        "#vnn.add(Dense(units = 9, activation = VNNACTIVATIONFXN))\n",
        "#vnn.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n",
        "\n",
        "#building cnn model\n",
        "CNNACTIVATIONFXN = 'sigmoid'\n",
        "cnn = Sequential();\n",
        "cnn.add(Conv1D(32, kernel_size=5, activation='relu', input_shape=(100,22)))\n",
        "cnn.add(MaxPooling1D())\n",
        "#cnn.add(Conv1D(32, kernel_size=5, activation='relu'))\n",
        "#cnn.add(MaxPooling1D())\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(units = 64, activation = CNNACTIVATIONFXN))\n",
        "cnn.add(Dense(units = 32, activation = CNNACTIVATIONFXN))\n",
        "cnn.add(Dense(units = 32, activation = CNNACTIVATIONFXN))\n",
        "cnn.add(Dense(units = 9, activation = 'tanh'))\n",
        "cnn.compile(optimizer = 'rmsprop', loss = 'mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "#fitting model\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    #Enter your parameters\n",
        "    monitor='loss',\n",
        "    min_delta=0,\n",
        "    patience=2,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        "    start_from_epoch=0\n",
        ")\n",
        "\n",
        "#vnn_history = vnn.fit(X_train.reshape((len(X_train), 2200)), y_train, batch_size = 128, epochs = 10, callbacks = [], verbose = 1)\n",
        "cnn_history = cnn.fit(X_train, y_train, batch_size = 128, epochs = 10, callbacks = [early_stopping], verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_XUgxz7jxPo"
      },
      "outputs": [],
      "source": [
        "#model evaluation\n",
        "'''\n",
        "vnn_yhat_test = vnn.predict(X_test.reshape(len(X_test), 2200))\n",
        "vnn_yhat_test = np.where(vnn_yhat_test > 0.5, 1, 0)\n",
        "print(\"VNN accuracy:\", accuracy_score(y_test, vnn_yhat_test))\n",
        "\n",
        "i=0\n",
        "vnn_yhat_test_classes = np.transpose(vnn_yhat_test)\n",
        "for hazard in y_test:\n",
        "    print(hazard, \"precision:\", precision(y_test[hazard], vnn_yhat_test_classes[i]))\n",
        "    print(hazard, \"recall:\", recall(y_test[hazard], vnn_yhat_test_classes[i]))\n",
        "    print()\n",
        "    i += 1\n",
        "''' #in general the vnn performs worse than the cnn\n",
        "\n",
        "cnn_yhat_test = cnn.predict(X_test)\n",
        "cnn_yhat_test = np.where(cnn_yhat_test > 0.5, 1, 0)\n",
        "print(\"\\nCNN accuracy:\", accuracy_score(y_test, cnn_yhat_test))\n",
        "i=0\n",
        "cnn_yhat_test_classes = np.transpose(cnn_yhat_test)\n",
        "for hazard in y_test:\n",
        "    print(hazard, \"precision:\", precision(y_test[hazard], cnn_yhat_test_classes[i]))\n",
        "    print(hazard, \"recall:\", recall(y_test[hazard], cnn_yhat_test_classes[i]))\n",
        "    print()\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning the CNN:"
      ],
      "metadata": {
        "id": "KS_gtH6wkbDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F00rOwgLjxPo",
        "outputId": "3887e4bf-ed7f-48bd-cb1a-7b3cade71507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloading Tuner from ./final_tuning/tuner0.json\n",
            "Search space summary\n",
            "Default search space size: 4\n",
            "activation (Choice)\n",
            "{'default': 'sigmoid', 'conditions': [], 'values': ['sigmoid', 'tanh', 'relu'], 'ordered': False}\n",
            "output activation (Choice)\n",
            "{'default': 'sigmoid', 'conditions': [], 'values': ['sigmoid', 'tanh'], 'ordered': False}\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 2, 'sampling': 'log'}\n",
            "num_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "# tuning\n",
        "\n",
        "def buildHPmodel(hp):\n",
        "    activation= hp.Choice(\"activation\", [\"sigmoid\", \"tanh\",'relu'])\n",
        "    finalactivation= hp.Choice(\"output activation\", [\"sigmoid\", \"tanh\"])\n",
        "    units =  int(hp.Int(\"units\", min_value=16, max_value=64, step=2, sampling='log'))\n",
        "\n",
        "    cnn = Sequential();\n",
        "    cnn.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(100,22)))\n",
        "    cnn.add(MaxPooling1D())\n",
        "    cnn.add(Flatten())\n",
        "\n",
        "    for i in range(hp.Int(\"num_layers\", 2, 4)):\n",
        "        cnn.add(Dense(units=units, activation=activation))\n",
        "\n",
        "    cnn.add(Dense(units = 9, activation = finalactivation))\n",
        "    opt = keras.optimizers.RMSprop(learning_rate = 0.05)\n",
        "    cnn.compile(optimizer = opt, loss = 'mean_squared_error', metrics=['accuracy'])\n",
        "    return cnn\n",
        "\n",
        "hp = keras_tuner.HyperParameters()\n",
        "\n",
        "tuner = keras_tuner.GridSearch(\n",
        "    hypermodel=buildHPmodel,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=55,\n",
        "    seed=15,\n",
        "    executions_per_trial=5,\n",
        "    #hyperparameters=None,\n",
        "    tune_new_entries=True,\n",
        "    allow_new_entries=True,\n",
        "    #max_retries_per_trial=0,\n",
        "    max_consecutive_failed_trials=3,\n",
        "    overwrite=False,\n",
        "    project_name=\"final_tuning\"\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chJOVTCUjxPo"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaSCf1AJjxPo",
        "outputId": "c55cc13c-6bc6-4d82-be35-c2d3f3e99ec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./final_tuning\n",
            "Showing 5 best trials\n",
            "Objective(name=\"val_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "activation: sigmoid\n",
            "output activation: tanh\n",
            "units: 16\n",
            "num_layers: 2\n",
            "Score: 0.7000060319900513\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "activation: sigmoid\n",
            "output activation: sigmoid\n",
            "units: 32.0\n",
            "num_layers: 2\n",
            "Score: 0.6946195363998413\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "activation: sigmoid\n",
            "output activation: sigmoid\n",
            "units: 64.0\n",
            "num_layers: 2\n",
            "Score: 0.6930386066436768\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "activation: sigmoid\n",
            "output activation: sigmoid\n",
            "units: 16\n",
            "num_layers: 3\n",
            "Score: 0.6900858879089355\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "activation: sigmoid\n",
            "output activation: sigmoid\n",
            "units: 32.0\n",
            "num_layers: 3\n",
            "Score: 0.6893859148025513\n"
          ]
        }
      ],
      "source": [
        "tuner.search(X_train, y_train, epochs=5, batch_size = 512, validation_data=(X_val, y_val))\n",
        "tuner.results_summary(num_trials=5)\n",
        "hps = tuner.get_best_hyperparameters(num_trials=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training up the top 5 models with more epochs to see which work best"
      ],
      "metadata": {
        "id": "pwHafu6lke1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "YflJdU6SjxPo"
      },
      "outputs": [],
      "source": [
        "\n",
        "num=0\n",
        "X_train, X_test, y_train, y_test = train_test_split(smilesnp, y, test_size=0.2, random_state=21)\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    #Enter your parameters\n",
        "    monitor='loss',\n",
        "    min_delta=0,\n",
        "    patience=2,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        "    start_from_epoch=0\n",
        ")\n",
        "\n",
        "for hp in hps:\n",
        "    print('Model', num, \":\\n\")\n",
        "    model = tuner.hypermodel.build(hp)\n",
        "    model_history = model.fit(X_train, y_train, batch_size = 128, epochs = 30, verbose = 1)\n",
        "    yhat_test = model.predict(X_test)\n",
        "    yhat_test = np.where(yhat_test > 0.5, 1, 0)\n",
        "    print(\"\\nAccuracy:\", accuracy_score(y_test, yhat_test))\n",
        "    i=0\n",
        "    yhat_test_classes = np.transpose(yhat_test)\n",
        "    for hazard in y_test:\n",
        "        print(hazard, \"precision:\", precision(y_test[hazard], yhat_test_classes[i]))\n",
        "        print(hazard, \"recall:\", recall(y_test[hazard], yhat_test_classes[i]))\n",
        "        print()\n",
        "        i += 1\n",
        "    num += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results aren't shown (i can rerun if necessary, but they all had accuracy ~65 +/- 5%, models with more layers+more units had better results on both test and training + having all activation functions as sigmoid yielded best results)\n",
        "\n",
        "Training the final model:"
      ],
      "metadata": {
        "id": "_a-DbJCzkljQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbsEqCrSjxPp",
        "outputId": "83fc828f-a16a-4b54-960a-321dad9fe92b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yxie10/.local/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.5585 - loss: 0.0919\n",
            "Epoch 2/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.5795 - loss: 0.0800\n",
            "Epoch 3/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - accuracy: 0.5780 - loss: 0.0800\n",
            "Epoch 4/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.5814 - loss: 0.0790\n",
            "Epoch 5/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.5883 - loss: 0.0734\n",
            "Epoch 6/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.6166 - loss: 0.0717\n",
            "Epoch 7/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.6208 - loss: 0.0706\n",
            "Epoch 8/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.6265 - loss: 0.0687\n",
            "Epoch 9/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6380 - loss: 0.0666\n",
            "Epoch 10/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.6460 - loss: 0.0648\n",
            "Epoch 11/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.6463 - loss: 0.0637\n",
            "Epoch 12/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - accuracy: 0.6521 - loss: 0.0622\n",
            "Epoch 13/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.6522 - loss: 0.0615\n",
            "Epoch 14/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6586 - loss: 0.0602\n",
            "Epoch 15/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.6633 - loss: 0.0588\n",
            "Epoch 16/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6691 - loss: 0.0580\n",
            "Epoch 17/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6747 - loss: 0.0567\n",
            "Epoch 18/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6781 - loss: 0.0558\n",
            "Epoch 19/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6814 - loss: 0.0547\n",
            "Epoch 20/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6836 - loss: 0.0540\n",
            "Epoch 21/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.6890 - loss: 0.0535\n",
            "Epoch 22/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.6911 - loss: 0.0533\n",
            "Epoch 23/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - accuracy: 0.6977 - loss: 0.0518\n",
            "Epoch 24/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6998 - loss: 0.0512\n",
            "Epoch 25/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7014 - loss: 0.0508\n",
            "Epoch 26/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7063 - loss: 0.0499\n",
            "Epoch 27/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7053 - loss: 0.0496\n",
            "Epoch 28/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7085 - loss: 0.0488\n",
            "Epoch 29/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7123 - loss: 0.0481\n",
            "Epoch 30/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7118 - loss: 0.0476\n",
            "Epoch 31/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7128 - loss: 0.0471\n",
            "Epoch 32/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7139 - loss: 0.0466\n",
            "Epoch 33/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7128 - loss: 0.0462\n",
            "Epoch 34/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7149 - loss: 0.0459\n",
            "Epoch 35/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7174 - loss: 0.0451\n",
            "Epoch 36/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7185 - loss: 0.0448\n",
            "Epoch 37/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7156 - loss: 0.0446\n",
            "Epoch 38/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7168 - loss: 0.0442\n",
            "Epoch 39/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7168 - loss: 0.0437\n",
            "Epoch 40/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.7196 - loss: 0.0433\n",
            "Epoch 41/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7211 - loss: 0.0430\n",
            "Epoch 42/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7218 - loss: 0.0424\n",
            "Epoch 43/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7230 - loss: 0.0422\n",
            "Epoch 44/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7225 - loss: 0.0424\n",
            "Epoch 45/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7227 - loss: 0.0420\n",
            "Epoch 46/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7237 - loss: 0.0416\n",
            "Epoch 47/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7243 - loss: 0.0413\n",
            "Epoch 48/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7259 - loss: 0.0410\n",
            "Epoch 49/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.7268 - loss: 0.0407\n",
            "Epoch 50/50\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7255 - loss: 0.0404\n",
            "\u001b[1m1554/1554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "\n",
            "Accuracy: 0.6529356155842067\n",
            "explosive precision: 0.9259615384615385\n",
            "explosive recall: 0.9219722355193872\n",
            "\n",
            "flammable precision: 0.7239888854584748\n",
            "flammable recall: 0.6575995513180034\n",
            "\n",
            "oxidizer precision: 0.9648093841642229\n",
            "oxidizer recall: 0.951617144359716\n",
            "\n",
            "pressurized precision: 0.9327485380116959\n",
            "pressurized recall: 0.7435897435897436\n",
            "\n",
            "corrosive precision: 0.72564804379327\n",
            "corrosive recall: 0.539050352828609\n",
            "\n",
            "toxic precision: 0.7147335423197492\n",
            "toxic recall: 0.2304776345716452\n",
            "\n",
            "irritant precision: 0.8797600811236488\n",
            "irritant recall: 0.9777244934660113\n",
            "\n",
            "health hazard precision: 0.7109185441941075\n",
            "health hazard recall: 0.44625761531766756\n",
            "\n",
            "environmental hazard precision: 0.6865263733260454\n",
            "environmental hazard recall: 0.48833592534992226\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# looks like more layer + more units per layer ends up yielding better results on both test and train data, so we'll try that ?\n",
        "# still not really sure how to improve recall over precision though...\n",
        "\n",
        "CNNACTIVATIONFXN = 'sigmoid'\n",
        "final_cnn = Sequential();\n",
        "final_cnn.add(Conv1D(32, kernel_size=5, activation='relu', input_shape=(100,22)))\n",
        "final_cnn.add(MaxPooling1D())\n",
        "#final_cnn.add(Conv1D(32, kernel_size=5, activation='relu'))\n",
        "#final_cnn.add(MaxPooling1D())\n",
        "final_cnn.add(Flatten())\n",
        "final_cnn.add(Dense(units = 64, activation = CNNACTIVATIONFXN))\n",
        "final_cnn.add(Dense(units = 64, activation = CNNACTIVATIONFXN))\n",
        "final_cnn.add(Dense(units = 64, activation = CNNACTIVATIONFXN))\n",
        "final_cnn.add(Dense(units = 9, activation = 'sigmoid'))\n",
        "final_cnn.compile(optimizer = 'rmsprop', loss = 'mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "model_history = final_cnn.fit(X_train, y_train, batch_size = 128, epochs = 50, verbose = 1)\n",
        "yhat_test = final_cnn.predict(X_test)\n",
        "yhat_test = np.where(yhat_test > 0.5, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5VEMxssjxPp",
        "outputId": "13ce3a9c-bde1-48bf-ccc2-0fb0036c1ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Overall Accuracy: 0.6529356155842067\n",
            "explosive accuracy: 0.9936239113381741\n",
            "explosive precision: 0.9259615384615385\n",
            "explosive recall: 0.9219722355193872\n",
            "\n",
            "flammable accuracy: 0.957459219180562\n",
            "flammable precision: 0.7239888854584748\n",
            "flammable recall: 0.6575995513180034\n",
            "\n",
            "oxidizer accuracy: 0.9936440251825331\n",
            "oxidizer precision: 0.9648093841642229\n",
            "oxidizer recall: 0.951617144359716\n",
            "\n",
            "pressurized accuracy: 0.9973248587002433\n",
            "pressurized precision: 0.9327485380116959\n",
            "pressurized recall: 0.7435897435897436\n",
            "\n",
            "corrosive accuracy: 0.8882072530522759\n",
            "corrosive precision: 0.72564804379327\n",
            "corrosive recall: 0.539050352828609\n",
            "\n",
            "toxic accuracy: 0.9314319045799223\n",
            "toxic precision: 0.7147335423197492\n",
            "toxic recall: 0.2304776345716452\n",
            "\n",
            "irritant accuracy: 0.8692197839773116\n",
            "irritant precision: 0.8797600811236488\n",
            "irritant recall: 0.9777244934660113\n",
            "\n",
            "health hazard accuracy: 0.9320353199106945\n",
            "health hazard precision: 0.7109185441941075\n",
            "health hazard recall: 0.44625761531766756\n",
            "\n",
            "environmental hazard accuracy: 0.9239897821670656\n",
            "environmental hazard precision: 0.6865263733260454\n",
            "environmental hazard recall: 0.48833592534992226\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nOverall Accuracy:\", accuracy_score(y_test, yhat_test))\n",
        "i=0\n",
        "yhat_test_classes = np.transpose(yhat_test)\n",
        "for hazard in y_test:\n",
        "    print(hazard, \"accuracy:\", accuracy_score(y_test[hazard], yhat_test_classes[i]))\n",
        "    print(hazard, \"precision:\", precision(y_test[hazard], yhat_test_classes[i]))\n",
        "    print(hazard, \"recall:\", recall(y_test[hazard], yhat_test_classes[i]))\n",
        "    print()\n",
        "    i += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}